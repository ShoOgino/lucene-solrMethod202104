  public SloppyPhraseMatcher(
      PhraseQuery.PostingsAndFreq[] postings,
      int slop,
      ScoreMode scoreMode,
      SimScorer scorer,
      float matchCost,
      boolean captureLeadMatch) {
    super(matchCost);
    this.slop = slop;
    this.numPostings = postings.length;
    this.captureLeadMatch = captureLeadMatch;
    pq = new PhraseQueue(postings.length);
    phrasePositions = new PhrasePositions[postings.length];
    for (int i = 0; i < postings.length; ++i) {
      phrasePositions[i] =
          new PhrasePositions(postings[i].postings, postings[i].position, i, postings[i].terms);
    }

    approximation =
        ConjunctionDISI.intersectIterators(
            Arrays.stream(postings).map(p -> p.postings).collect(Collectors.toList()));
    // What would be a good upper bound of the sloppy frequency? A sum of the
    // sub frequencies would be correct, but it is usually so much higher than
    // the actual sloppy frequency that it doesn't help skip irrelevant
    // documents. As a consequence for now, sloppy phrase queries use dummy
    // impacts:
    final ImpactsSource impactsSource =
        new ImpactsSource() {
          @Override
          public Impacts getImpacts() throws IOException {
            return new Impacts() {

              @Override
              public int numLevels() {
                return 1;
              }

              @Override
              public List<Impact> getImpacts(int level) {
                return Collections.singletonList(new Impact(Integer.MAX_VALUE, 1L));
              }

              @Override
              public int getDocIdUpTo(int level) {
                return DocIdSetIterator.NO_MORE_DOCS;
              }
            };
          }

          @Override
          public void advanceShallow(int target) throws IOException {}
        };
    impactsApproximation = new ImpactsDISI(approximation, impactsSource, scorer);
  }

