  private final void mergeTerms() throws CorruptIndexException, IOException {

    // Let CodecProvider decide which codec will be used to write
    // the new segment:

    int docBase = 0;

    final List<Fields> fields = new ArrayList<Fields>();
    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();
    final List<Bits> bits = new ArrayList<Bits>();
    final List<Integer> bitsStarts = new ArrayList<Integer>();

    for(IndexReader r : readers) {
      final Fields f = r.fields();
      final int maxDoc = r.maxDoc();
      if (f != null) {
        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));
        fields.add(f);
        bits.add(r.getDeletedDocs());
        bitsStarts.add(docBase);
      }
      docBase += maxDoc;
    }

    bitsStarts.add(docBase);

    // we may gather more readers than mergeState.readerCount
    mergeState = new MergeState();
    mergeState.readers = readers;
    mergeState.readerCount = readers.size();
    mergeState.fieldInfos = fieldInfos;
    mergeState.mergedDocCount = mergedDocs;

    // Remap docIDs
    mergeState.delCounts = new int[mergeState.readerCount];
    mergeState.docMaps = new int[mergeState.readerCount][];
    mergeState.docBase = new int[mergeState.readerCount];
    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;
    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];
    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];
    mergeState.checkAbort = checkAbort;

    docBase = 0;
    int inputDocBase = 0;

    for(int i=0;i<mergeState.readerCount;i++) {

      final IndexReader reader = readers.get(i);

      mergeState.delCounts[i] = reader.numDeletedDocs();
      mergeState.docBase[i] = docBase;
      docBase += reader.numDocs();
      inputDocBase += reader.maxDoc();
      if (mergeState.delCounts[i] != 0) {
        int delCount = 0;
        final Bits delDocs = reader.getDeletedDocs();
        assert delDocs != null;
        final int maxDoc = reader.maxDoc();
        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];
        int newDocID = 0;
        for(int j=0;j<maxDoc;j++) {
          if (delDocs.get(j)) {
            docMap[j] = -1;
            delCount++;  // only for assert
          } else {
            docMap[j] = newDocID++;
          }
        }
        assert delCount == mergeState.delCounts[i]: "reader delCount=" + mergeState.delCounts[i] + " vs recomputed delCount=" + delCount;
      }

      if (payloadProcessorProvider != null) {
        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());
      }
    }
    codec = segmentWriteState.segmentCodecs.codec();
    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);

    // NOTE: this is silly, yet, necessary -- we create a
    // MultiBits as our skip docs only to have it broken
    // apart when we step through the docs enums in
    // MultiDocsEnum.
    mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);

    try {
      consumer.merge(mergeState,
                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),
                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));
    } finally {
      consumer.close();
    }
  }

