  // LUCENE-3642
  // EdgeNgram blindly adds term length to offset, but this can take things out of bounds
  // wrt original text if a previous filter increases the length of the word (in this case æ -> ae)
  // so in this case we behave like WDF, and preserve any modified offsets
  public void testInvalidOffsets() throws Exception {
    Analyzer analyzer =
        new Analyzer() {
          @Override
          protected TokenStreamComponents createComponents(String fieldName) {
            Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);
            TokenFilter filters = new ASCIIFoldingFilter(tokenizer);
            filters = new NGramTokenFilter(filters, 2, 2, false);
            return new TokenStreamComponents(tokenizer, filters);
          }
        };
    assertAnalyzesTo(
        analyzer,
        "mosfellsbær",
        new String[] {"mo", "os", "sf", "fe", "el", "ll", "ls", "sb", "ba", "ae", "er"},
        new int[] {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0},
        new int[] {11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11},
        new int[] {1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0});
    analyzer.close();
  }

