  /**
   * Carefully merges deletes and updates for the segments we just merged. This is tricky because,
   * although merging will clear all deletes (compacts the documents) and compact all the updates,
   * new deletes and updates may have been flushed to the segments since the merge was started. This
   * method "carries over" such new deletes and updates onto the newly merged segment, and saves the
   * resulting deletes and updates files (incrementing the delete and DV generations for
   * merge.info). If no deletes were flushed, no new deletes file is saved.
   */
  private synchronized ReadersAndUpdates commitMergedDeletesAndUpdates(
      MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {

    mergeFinishedGen.incrementAndGet();

    testPoint("startCommitMergeDeletes");

    final List<SegmentCommitInfo> sourceSegments = merge.segments;

    if (infoStream.isEnabled("IW")) {
      infoStream.message("IW", "commitMergeDeletes " + segString(merge.segments));
    }

    // Carefully merge deletes that occurred after we
    // started merging:
    long minGen = Long.MAX_VALUE;

    // Lazy init (only when we find a delete or update to carry over):
    final ReadersAndUpdates mergedDeletesAndUpdates = getPooledInstance(merge.info, true);
    int numDeletesBefore = mergedDeletesAndUpdates.getDelCount();
    // field -> delGen -> dv field updates
    Map<String, Map<Long, DocValuesFieldUpdates>> mappedDVUpdates = new HashMap<>();

    boolean anyDVUpdates = false;

    assert sourceSegments.size() == mergeState.docMaps.length;
    for (int i = 0; i < sourceSegments.size(); i++) {
      SegmentCommitInfo info = sourceSegments.get(i);
      minGen = Math.min(info.getBufferedDeletesGen(), minGen);
      final int maxDoc = info.info.maxDoc();
      final ReadersAndUpdates rld = getPooledInstance(info, false);
      // We hold a ref, from when we opened the readers during mergeInit, so it better still be in
      // the pool:
      assert rld != null : "seg=" + info.info.name;

      MergeState.DocMap segDocMap = mergeState.docMaps[i];
      MergeState.DocMap segLeafDocMap = mergeState.leafDocMaps[i];

      carryOverHardDeletes(
          mergedDeletesAndUpdates,
          maxDoc,
          mergeState.liveDocs[i],
          merge.getMergeReader().get(i).hardLiveDocs,
          rld.getHardLiveDocs(),
          segDocMap,
          segLeafDocMap);

      // Now carry over all doc values updates that were resolved while we were merging, remapping
      // the docIDs to the newly merged docIDs.
      // We only carry over packets that finished resolving; if any are still running (concurrently)
      // they will detect that our merge completed
      // and re-resolve against the newly merged segment:
      Map<String, List<DocValuesFieldUpdates>> mergingDVUpdates = rld.getMergingDVUpdates();
      for (Map.Entry<String, List<DocValuesFieldUpdates>> ent : mergingDVUpdates.entrySet()) {

        String field = ent.getKey();

        Map<Long, DocValuesFieldUpdates> mappedField = mappedDVUpdates.get(field);
        if (mappedField == null) {
          mappedField = new HashMap<>();
          mappedDVUpdates.put(field, mappedField);
        }

        for (DocValuesFieldUpdates updates : ent.getValue()) {

          if (bufferedUpdatesStream.stillRunning(updates.delGen)) {
            continue;
          }

          // sanity check:
          assert field.equals(updates.field);

          DocValuesFieldUpdates mappedUpdates = mappedField.get(updates.delGen);
          if (mappedUpdates == null) {
            switch (updates.type) {
              case NUMERIC:
                mappedUpdates =
                    new NumericDocValuesFieldUpdates(
                        updates.delGen, updates.field, merge.info.info.maxDoc());
                break;
              case BINARY:
                mappedUpdates =
                    new BinaryDocValuesFieldUpdates(
                        updates.delGen, updates.field, merge.info.info.maxDoc());
                break;
              default:
                throw new AssertionError();
            }
            mappedField.put(updates.delGen, mappedUpdates);
          }

          DocValuesFieldUpdates.Iterator it = updates.iterator();
          int doc;
          while ((doc = it.nextDoc()) != NO_MORE_DOCS) {
            int mappedDoc = segDocMap.get(segLeafDocMap.get(doc));
            if (mappedDoc != -1) {
              if (it.hasValue()) {
                // not deleted
                mappedUpdates.add(mappedDoc, it);
              } else {
                mappedUpdates.reset(mappedDoc);
              }
              anyDVUpdates = true;
            }
          }
        }
      }
    }

    if (anyDVUpdates) {
      // Persist the merged DV updates onto the RAU for the merged segment:
      for (Map<Long, DocValuesFieldUpdates> d : mappedDVUpdates.values()) {
        for (DocValuesFieldUpdates updates : d.values()) {
          updates.finish();
          mergedDeletesAndUpdates.addDVUpdate(updates);
        }
      }
    }

    if (infoStream.isEnabled("IW")) {
      if (mergedDeletesAndUpdates == null) {
        infoStream.message("IW", "no new deletes or field updates since merge started");
      } else {
        String msg = mergedDeletesAndUpdates.getDelCount() - numDeletesBefore + " new deletes";
        if (anyDVUpdates) {
          msg += " and " + mergedDeletesAndUpdates.getNumDVUpdates() + " new field updates";
          msg += " (" + mergedDeletesAndUpdates.ramBytesUsed.get() + ") bytes";
        }
        msg += " since merge started";
        infoStream.message("IW", msg);
      }
    }

    merge.info.setBufferedDeletesGen(minGen);

    return mergedDeletesAndUpdates;
  }

