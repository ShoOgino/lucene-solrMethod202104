  @Override
  public int getPartition(Text key, SolrInputDocumentWritable value, int numPartitions) {
    DocRouter docRouter = docCollection.getRouter();
    SolrInputDocument doc = value.getSolrInputDocument();
    String keyStr = key.toString();
    
    // TODO: scalability: replace linear search in HashBasedRouter.hashToSlice() with binary search on sorted hash ranges
    Slice slice = docRouter.getTargetSlice(keyStr, doc, null, emptySolrParams, docCollection); 
    
//    LOG.info("slice: {}", slice);
    if (slice == null) {
      throw new IllegalStateException("No matching slice found! The slice seems unavailable. docRouterClass: "
          + docRouter.getClass().getName());
    }
    int rootShard = shardNumbers.get(slice.getName());
    if (rootShard < 0 || rootShard >= shards) {
      throw new IllegalStateException("Illegal shard number " + rootShard + " for slice: " + slice + ", docCollection: "
          + docCollection);
    }      

    // map doc to micro shard aka leaf shard, akin to HashBasedRouter.sliceHash()
    // taking into account mtree merge algorithm
    assert numPartitions % shards == 0; // Also note that numPartitions is equal to the number of reducers
    int hashCode = Hash.murmurhash3_x86_32(keyStr, 0, keyStr.length(), 0); 
    int offset = (hashCode & Integer.MAX_VALUE) % (numPartitions / shards);
    int microShard = (rootShard * (numPartitions / shards)) + offset;
//    LOG.info("Subpartitions rootShard: {}, offset: {}", rootShard, offset);
//    LOG.info("Partitioned to p: {} for numPartitions: {}, shards: {}, key: {}, value: {}", microShard, numPartitions, shards, key, value);
    
    assert microShard >= 0 && microShard < numPartitions;
    return microShard;
  }

