  private static long applyDocValuesUpdates(BufferedUpdatesStream.SegmentState segState, byte[] updates,
                                            boolean isNumeric, long delGen,
                                            boolean segmentPrivateDeletes) throws IOException {

    TermsEnum termsEnum = null;
    PostingsEnum postingsEnum = null;

    // TODO: we can process the updates per DV field, from last to first so that
    // if multiple terms affect same document for the same field, we add an update
    // only once (that of the last term). To do that, we can keep a bitset which
    // marks which documents have already been updated. So e.g. if term T1
    // updates doc 7, and then we process term T2 and it updates doc 7 as well,
    // we don't apply the update since we know T1 came last and therefore wins
    // the update.
    // We can also use that bitset as 'liveDocs' to pass to TermEnum.docs(), so
    // that these documents aren't even returned.

    long updateCount = 0;

    // We first write all our updates private, and only in the end publish to the ReadersAndUpdates */
    Map<String, DocValuesFieldUpdates> holder = new HashMap<>();

    ByteArrayDataInput in = new ByteArrayDataInput(updates);

    String termField = null;
    String updateField = null;
    BytesRef term = new BytesRef();
    term.bytes = new byte[16];
    
    BytesRef scratch = new BytesRef();
    scratch.bytes = new byte[16];
    
    while (in.getPosition() != updates.length) {
      int code = in.readVInt();
      int docIDUpto = in.readVInt();
      term.length = code >> 3;
      
      if ((code & 1) != 0) {
        termField = in.readString();
      }
      if ((code & 2) != 0) {
        updateField = in.readString();
      }
      boolean hasValue = (code & 4) != 0;

      if (term.bytes.length < term.length) {
        term.bytes = ArrayUtil.grow(term.bytes, term.length);
      }
      in.readBytes(term.bytes, 0, term.length);

      final int limit;
      if (delGen == segState.delGen) {
        assert segmentPrivateDeletes;
        limit = docIDUpto;
      } else {
        limit = Integer.MAX_VALUE;
      }
        
      // TODO: we traverse the terms in update order (not term order) so that we
      // apply the updates in the correct order, i.e. if two terms udpate the
      // same document, the last one that came in wins, irrespective of the
      // terms lexical order.
      // we can apply the updates in terms order if we keep an updatesGen (and
      // increment it with every update) and attach it to each NumericUpdate. Note
      // that we cannot rely only on docIDUpto because an app may send two updates
      // which will get same docIDUpto, yet will still need to respect the order
      // those updates arrived.

      // TODO: we could at least *collate* by field?

      // This is the field used to resolve to docIDs, e.g. an "id" field, not the doc values field we are updating!
      if ((code & 1) != 0) {
        Terms terms = segState.reader.terms(termField);
        if (terms != null) {
          termsEnum = terms.iterator();
        } else {
          termsEnum = null;
        }
      }

      final BytesRef binaryValue;
      final long longValue;
      if (hasValue == false) {
        longValue = -1;
        binaryValue = null;
      } else if (isNumeric) {
        longValue = NumericDocValuesUpdate.readFrom(in);
        binaryValue = null;
      } else {
        longValue = -1;
        binaryValue = BinaryDocValuesUpdate.readFrom(in, scratch);
      }

      if (termsEnum == null) {
        // no terms in this segment for this field
        continue;
      }

      if (termsEnum.seekExact(term)) {
        // we don't need term frequencies for this
        postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);
        DocValuesFieldUpdates dvUpdates = holder.get(updateField);
        if (dvUpdates == null) {
          if (isNumeric) {
            dvUpdates = new NumericDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc());
          } else {
            dvUpdates = new BinaryDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc());
          }
          holder.put(updateField, dvUpdates);
        }
        final IntConsumer docIdConsumer;
        final DocValuesFieldUpdates update = dvUpdates;
        if (hasValue == false) {
          docIdConsumer = doc -> update.reset(doc);
        } else if (isNumeric) {
          docIdConsumer = doc -> update.add(doc, longValue);
        } else {
          docIdConsumer = doc -> update.add(doc, binaryValue);
        }
        final Bits acceptDocs = segState.rld.getLiveDocs();
        if (segState.rld.sortMap != null && segmentPrivateDeletes) {
          // This segment was sorted on flush; we must apply seg-private deletes carefully in this case:
          int doc;
          while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
            if (acceptDocs == null || acceptDocs.get(doc)) {
              // The limit is in the pre-sorted doc space:
              if (segState.rld.sortMap.newToOld(doc) < limit) {
                docIdConsumer.accept(doc);
                updateCount++;
              }
            }
          }
        } else {
          int doc;
          while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
            if (doc >= limit) {
              break; // no more docs that can be updated for this term
            }
            if (acceptDocs == null || acceptDocs.get(doc)) {
              docIdConsumer.accept(doc);
              updateCount++;
            }
          }
        }
      }
    }

    // now freeze & publish:
    for (DocValuesFieldUpdates update : holder.values()) {
      if (update.any()) {
        update.finish();
        segState.rld.addDVUpdate(update);
      }
    }

    return updateCount;
  }

