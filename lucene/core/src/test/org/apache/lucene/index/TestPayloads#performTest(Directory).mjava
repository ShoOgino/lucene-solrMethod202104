  // builds an index with payloads in the given Directory and performs
  // different tests to verify the payload encoding
  private void performTest(Directory dir) throws Exception {
    PayloadAnalyzer analyzer = new PayloadAnalyzer();
    IndexWriter writer =
        new IndexWriter(
            dir,
            newIndexWriterConfig(analyzer)
                .setOpenMode(OpenMode.CREATE)
                .setMergePolicy(newLogMergePolicy()));

    // should be in sync with value in TermInfosWriter
    final int skipInterval = 16;

    final int numTerms = 5;
    final String fieldName = "f1";

    int numDocs = skipInterval + 1;
    // create content for the test documents with just a few terms
    Term[] terms = generateTerms(fieldName, numTerms);
    StringBuilder sb = new StringBuilder();
    for (int i = 0; i < terms.length; i++) {
      sb.append(terms[i].text());
      sb.append(" ");
    }
    String content = sb.toString();

    int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;
    byte[] payloadData = generateRandomData(payloadDataLength);

    Document d = new Document();
    d.add(newTextField(fieldName, content, Field.Store.NO));
    // add the same document multiple times to have the same payload lengths for all
    // occurrences within two consecutive skip intervals
    int offset = 0;
    for (int i = 0; i < 2 * numDocs; i++) {
      analyzer.setPayloadData(fieldName, payloadData, offset, 1);
      offset += numTerms;
      writer.addDocument(d);
    }

    // make sure we create more than one segment to test merging
    writer.commit();

    // now we make sure to have different payload lengths next at the next skip point
    for (int i = 0; i < numDocs; i++) {
      analyzer.setPayloadData(fieldName, payloadData, offset, i);
      offset += i * numTerms;
      writer.addDocument(d);
    }

    writer.forceMerge(1);
    // flush
    writer.close();

    /*
     * Verify the index
     * first we test if all payloads are stored correctly
     */
    IndexReader reader = DirectoryReader.open(dir);

    byte[] verifyPayloadData = new byte[payloadDataLength];
    offset = 0;
    PostingsEnum[] tps = new PostingsEnum[numTerms];
    for (int i = 0; i < numTerms; i++) {
      tps[i] =
          MultiTerms.getTermPostingsEnum(reader, terms[i].field(), new BytesRef(terms[i].text()));
    }

    while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
      for (int i = 1; i < numTerms; i++) {
        tps[i].nextDoc();
      }
      int freq = tps[0].freq();

      for (int i = 0; i < freq; i++) {
        for (int j = 0; j < numTerms; j++) {
          tps[j].nextPosition();
          BytesRef br = tps[j].getPayload();
          if (br != null) {
            System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);
            offset += br.length;
          }
        }
      }
    }

    assertByteArrayEquals(payloadData, verifyPayloadData);

    /*
     *  test lazy skipping
     */
    PostingsEnum tp =
        MultiTerms.getTermPostingsEnum(reader, terms[0].field(), new BytesRef(terms[0].text()));
    tp.nextDoc();
    tp.nextPosition();
    // NOTE: prior rev of this test was failing to first
    // call next here:
    tp.nextDoc();
    // now we don't read this payload
    tp.nextPosition();
    BytesRef payload = tp.getPayload();
    assertEquals("Wrong payload length.", 1, payload.length);
    assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);
    tp.nextDoc();
    tp.nextPosition();

    // we don't read this payload and skip to a different document
    tp.advance(5);
    tp.nextPosition();
    payload = tp.getPayload();
    assertEquals("Wrong payload length.", 1, payload.length);
    assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);

    /*
     * Test different lengths at skip points
     */
    tp = MultiTerms.getTermPostingsEnum(reader, terms[1].field(), new BytesRef(terms[1].text()));
    tp.nextDoc();
    tp.nextPosition();
    assertEquals("Wrong payload length.", 1, tp.getPayload().length);
    tp.advance(skipInterval - 1);
    tp.nextPosition();
    assertEquals("Wrong payload length.", 1, tp.getPayload().length);
    tp.advance(2 * skipInterval - 1);
    tp.nextPosition();
    assertEquals("Wrong payload length.", 1, tp.getPayload().length);
    tp.advance(3 * skipInterval - 1);
    tp.nextPosition();
    assertEquals(
        "Wrong payload length.", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);

    reader.close();

    // test long payload
    analyzer = new PayloadAnalyzer();
    writer = new IndexWriter(dir, newIndexWriterConfig(analyzer).setOpenMode(OpenMode.CREATE));
    String singleTerm = "lucene";

    d = new Document();
    d.add(newTextField(fieldName, singleTerm, Field.Store.NO));
    // add a payload whose length is greater than the buffer size of BufferedIndexOutput
    payloadData = generateRandomData(2000);
    analyzer.setPayloadData(fieldName, payloadData, 100, 1500);
    writer.addDocument(d);

    writer.forceMerge(1);
    // flush
    writer.close();

    reader = DirectoryReader.open(dir);
    tp = MultiTerms.getTermPostingsEnum(reader, fieldName, new BytesRef(singleTerm));
    tp.nextDoc();
    tp.nextPosition();

    BytesRef br = tp.getPayload();
    verifyPayloadData = new byte[br.length];
    byte[] portion = new byte[1500];
    System.arraycopy(payloadData, 100, portion, 0, 1500);

    assertByteArrayEquals(portion, br.bytes, br.offset, br.length);
    reader.close();
  }

