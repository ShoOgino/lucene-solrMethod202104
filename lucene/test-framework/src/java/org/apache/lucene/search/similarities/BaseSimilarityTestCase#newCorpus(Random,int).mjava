  /**
   * returns a random corpus that is at least possible given the norm value for a single document.
   */
  static CollectionStatistics newCorpus(Random random, int norm) {
    // lower bound of tokens in the collection (you produced this norm somehow)
    final int lowerBound;
    if (norm == 0) {
      // norms are omitted, but there must have been at least one token to produce that norm
      lowerBound = 1;
    } else {
      // minimum value that would decode to such a norm
      lowerBound = SmallFloat.byte4ToInt((byte) norm);
    }
    final long maxDoc;
    switch (random.nextInt(6)) {
      case 0:
        // 1 doc collection
        maxDoc = 1;
        break;
      case 1:
        // 2 doc collection
        maxDoc = 2;
        break;
      case 2:
        // tiny collection
        maxDoc = TestUtil.nextLong(random, 3, 16);
        break;
      case 3:
        // small collection
        maxDoc = TestUtil.nextLong(random, 16, 100000);
        break;
      case 4:
        // big collection
        maxDoc = TestUtil.nextLong(random, 100000, MAXDOC_FORTESTING);
        break;
      default:
        // yuge collection
        maxDoc = MAXDOC_FORTESTING;
        break;
    }
    final long docCount;
    switch (random.nextInt(3)) {
      case 0:
        // sparsest field
        docCount = 1;
        break;
      case 1:
        // sparse field
        docCount = TestUtil.nextLong(random, 1, maxDoc);
        break;
      default:
        // fully populated
        docCount = maxDoc;
        break;
    }
    // random docsize: but can't require docs to have > 2B tokens
    long upperBound;
    try {
      upperBound = Math.min(MAXTOKENS_FORTESTING, Math.multiplyExact(docCount, Integer.MAX_VALUE));
    } catch (ArithmeticException overflow) {
      upperBound = MAXTOKENS_FORTESTING;
    }
    final long sumDocFreq;
    switch (random.nextInt(3)) {
      case 0:
        // shortest possible docs
        sumDocFreq = docCount;
        break;
      case 1:
        // biggest possible docs
        sumDocFreq = upperBound + 1 - lowerBound;
        break;
      default:
        // random docsize
        sumDocFreq = TestUtil.nextLong(random, docCount, upperBound + 1 - lowerBound);
        break;
    }
    final long sumTotalTermFreq;
    switch (random.nextInt(4)) {
      case 0:
        // term frequencies were omitted
        sumTotalTermFreq = sumDocFreq;
        break;
      case 1:
        // no repetition of terms (except to satisfy this norm)
        sumTotalTermFreq = sumDocFreq - 1 + lowerBound;
        break;
      case 2:
        // maximum repetition of terms
        sumTotalTermFreq = upperBound;
        break;
      default:
        // random repetition
        assert sumDocFreq - 1 + lowerBound <= upperBound;
        sumTotalTermFreq = TestUtil.nextLong(random, sumDocFreq - 1 + lowerBound, upperBound);
        break;
    }
    return new CollectionStatistics("field", maxDoc, docCount, sumTotalTermFreq, sumDocFreq);
  }

