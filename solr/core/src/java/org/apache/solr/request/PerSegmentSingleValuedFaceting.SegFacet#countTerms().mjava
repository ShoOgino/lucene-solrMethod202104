    void countTerms() throws IOException {
      si = DocValues.getSorted(context.reader(), fieldName);
      // SolrCore.log.info("reader= " + reader + "  FC=" + System.identityHashCode(si));

      if (prefix!=null) {
        BytesRefBuilder prefixRef = new BytesRefBuilder();
        prefixRef.copyChars(prefix);
        startTermIndex = si.lookupTerm(prefixRef.get());
        if (startTermIndex<0) startTermIndex=-startTermIndex-1;
        prefixRef.append(UnicodeUtil.BIG_TERM);
        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end
        endTermIndex = si.lookupTerm(prefixRef.get());
        assert endTermIndex < 0;
        endTermIndex = -endTermIndex-1;
      } else {
        startTermIndex=-1;
        endTermIndex=si.getValueCount();
      }
      final int nTerms=endTermIndex-startTermIndex;
      if (nTerms == 0) return;

      // count collection array only needs to be as big as the number of terms we are
      // going to collect counts for.
      final int[] counts = this.counts = new int[nTerms];
      DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs
      DocIdSetIterator iter = idSet.iterator();

      if (prefix==null) {
        // specialized version when collecting counts for all terms
        int doc;
        while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {
          counts[1+si.getOrd(doc)]++;
        }
      } else {
        // version that adjusts term numbers because we aren't collecting the full range
        int doc;
        while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {
          int term = si.getOrd(doc);
          int arrIdx = term-startTermIndex;
          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;
        }
      }
    }

