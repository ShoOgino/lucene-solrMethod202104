  /**
   * Expert: returns a readonly reader, covering all committed as well as un-committed changes to
   * the index. This provides "near real-time" searching, in that changes made during an IndexWriter
   * session can be quickly made available for searching without closing the writer nor calling
   * {@link #commit}.
   *
   * <p>Note that this is functionally equivalent to calling {#flush} and then opening a new reader.
   * But the turnaround time of this method should be faster since it avoids the potentially costly
   * {@link #commit}.
   *
   * <p>You must close the {@link IndexReader} returned by this method once you are done using it.
   *
   * <p>It's <i>near</i> real-time because there is no hard guarantee on how quickly you can get a
   * new reader after making changes with IndexWriter. You'll have to experiment in your situation
   * to determine if it's fast enough. As this is a new and experimental feature, please report back
   * on your findings so we can learn, improve and iterate.
   *
   * <p>The resulting reader supports {@link DirectoryReader#openIfChanged}, but that call will
   * simply forward back to this method (though this may change in the future).
   *
   * <p>The very first time this method is called, this writer instance will make every effort to
   * pool the readers that it opens for doing merges, applying deletes, etc. This means additional
   * resources (RAM, file descriptors, CPU time) will be consumed.
   *
   * <p>For lower latency on reopening a reader, you should call {@link
   * IndexWriterConfig#setMergedSegmentWarmer} to pre-warm a newly merged segment before it's
   * committed to the index. This is important for minimizing index-to-search delay after a large
   * merge.
   *
   * <p>If an addIndexes* call is running in another thread, then this reader will only search those
   * segments from the foreign index that have been successfully copied over, so far.
   *
   * <p><b>NOTE</b>: Once the writer is closed, any outstanding readers may continue to be used.
   * However, if you attempt to reopen any of those readers, you'll hit an {@link
   * AlreadyClosedException}.
   *
   * @lucene.experimental
   * @return IndexReader that covers entire index plus all changes made so far by this IndexWriter
   *     instance
   * @throws IOException If there is a low-level I/O error
   */
  DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {
    ensureOpen();

    if (writeAllDeletes && applyAllDeletes == false) {
      throw new IllegalArgumentException("applyAllDeletes must be true when writeAllDeletes=true");
    }

    final long tStart = System.currentTimeMillis();

    if (infoStream.isEnabled("IW")) {
      infoStream.message("IW", "flush at getReader");
    }
    // Do this up front before flushing so that the readers
    // obtained during this flush are pooled, the first time
    // this method is called:
    readerPool.enableReaderPooling();
    StandardDirectoryReader r = null;
    doBeforeFlush();
    boolean anyChanges;
    final long maxFullFlushMergeWaitMillis = config.getMaxFullFlushMergeWaitMillis();
    /*
     * for releasing a NRT reader we must ensure that
     * DW doesn't add any segments or deletes until we are
     * done with creating the NRT DirectoryReader.
     * We release the two stage full flush after we are done opening the
     * directory reader!
     */
    MergePolicy.MergeSpecification onGetReaderMerges = null;
    final AtomicBoolean stopCollectingMergedReaders = new AtomicBoolean(false);
    final Map<String, SegmentReader> mergedReaders = new HashMap<>();
    final Map<String, SegmentReader> openedReadOnlyClones = new HashMap<>();
    // this function is used to control which SR are opened in order to keep track of them
    // and to reuse them in the case we wait for merges in this getReader call.
    IOUtils.IOFunction<SegmentCommitInfo, SegmentReader> readerFactory =
        sci -> {
          final ReadersAndUpdates rld = getPooledInstance(sci, true);
          try {
            assert Thread.holdsLock(IndexWriter.this);
            SegmentReader segmentReader = rld.getReadOnlyClone(IOContext.READ);
            // only track this if we actually do fullFlush merges
            if (maxFullFlushMergeWaitMillis > 0) {
              openedReadOnlyClones.put(sci.info.name, segmentReader);
            }
            return segmentReader;
          } finally {
            release(rld);
          }
        };
    Closeable onGetReaderMergeResources = null;
    SegmentInfos openingSegmentInfos = null;
    boolean success2 = false;
    try {
      /* This is the essential part of the getReader method. We need to take care of the following things:
       *  - flush all currently in-memory DWPTs to disk
       *  - apply all deletes & updates to new and to the existing DWPTs
       *  - prevent flushes and applying deletes of concurrently indexing DWPTs to be applied
       *  - open a SDR on the updated SIS
       *
       * in order to prevent concurrent flushes we call DocumentsWriter#flushAllThreads that swaps out the deleteQueue
       *  (this enforces a happens before relationship between this and the subsequent full flush) and informs the
       * FlushControl (#markForFullFlush()) that it should prevent any new DWPTs from flushing until we are \
       * done (DocumentsWriter#finishFullFlush(boolean)). All this is guarded by the fullFlushLock to prevent multiple
       * full flushes from happening concurrently. Once the DocWriter has initiated a full flush we can sequentially flush
       * and apply deletes & updates to the written segments without worrying about concurrently indexing DWPTs. The important
       * aspect is that it all happens between DocumentsWriter#flushAllThread() and DocumentsWriter#finishFullFlush(boolean)
       * since once the flush is marked as done deletes start to be applied to the segments on disk without guarantees that
       * the corresponding added documents (in the update case) are flushed and visible when opening a SDR.
       */
      boolean success = false;
      synchronized (fullFlushLock) {
        try {
          // TODO: should we somehow make the seqNo available in the returned NRT reader?
          anyChanges = docWriter.flushAllThreads() < 0;
          if (anyChanges == false) {
            // prevent double increment since docWriter#doFlush increments the flushcount
            // if we flushed anything.
            flushCount.incrementAndGet();
          }
          publishFlushedSegments(true);
          processEvents(false);

          if (applyAllDeletes) {
            applyAllDeletesAndUpdates();
          }
          synchronized (this) {

            // NOTE: we cannot carry doc values updates in memory yet, so we always must write them
            // through to disk and re-open each
            // SegmentReader:

            // TODO: we could instead just clone SIS and pull/incref readers in sync'd block, and
            // then do this w/o IW's lock?
            // Must do this sync'd on IW to prevent a merge from completing at the last second and
            // failing to write its DV updates:
            writeReaderPool(writeAllDeletes);

            // Prevent segmentInfos from changing while opening the
            // reader; in theory we could instead do similar retry logic,
            // just like we do when loading segments_N
            r =
                StandardDirectoryReader.open(
                    this, readerFactory, segmentInfos, applyAllDeletes, writeAllDeletes);
            if (infoStream.isEnabled("IW")) {
              infoStream.message("IW", "return reader version=" + r.getVersion() + " reader=" + r);
            }
            if (maxFullFlushMergeWaitMillis > 0) {
              // we take the SIS from the reader which has already pruned away fully deleted readers
              // this makes pulling the readers below after the merge simpler since we can be safe
              // that
              // they are not closed. Every segment has a corresponding SR in the SDR we opened if
              // we use
              // this SIS
              // we need to do this rather complicated management of SRs and infos since we can't
              // wait for merges
              // while we hold the fullFlushLock since the merge might hit a tragic event and that
              // must not be reported
              // while holding that lock. Merging outside of the lock ie. after calling
              // docWriter.finishFullFlush(boolean) would
              // yield wrong results because deletes might sneak in during the merge
              openingSegmentInfos = r.getSegmentInfos().clone();
              onGetReaderMerges =
                  preparePointInTimeMerge(
                      openingSegmentInfos,
                      stopCollectingMergedReaders::get,
                      MergeTrigger.GET_READER,
                      sci -> {
                        assert stopCollectingMergedReaders.get() == false
                            : "illegal state  merge reader must be not pulled since we already stopped waiting for merges";
                        SegmentReader apply = readerFactory.apply(sci);
                        mergedReaders.put(sci.info.name, apply);
                        // we need to incRef the files of the opened SR otherwise it's possible that
                        // another merge
                        // removes the segment before we pass it on to the SDR
                        deleter.incRef(sci.files());
                      });
              onGetReaderMergeResources =
                  () -> {
                    // this needs to be closed once after we are done. In the case of an exception
                    // it releases
                    // all resources, closes the merged readers and decrements the files references.
                    // this only happens for readers that haven't been removed from the
                    // mergedReaders and release elsewhere
                    synchronized (this) {
                      stopCollectingMergedReaders.set(true);
                      IOUtils.close(
                          mergedReaders.values().stream()
                              .map(
                                  sr ->
                                      (Closeable)
                                          () -> {
                                            try {
                                              deleter.decRef(sr.getSegmentInfo().files());
                                            } finally {
                                              sr.close();
                                            }
                                          })
                              .collect(Collectors.toList()));
                    }
                  };
            }
          }
          success = true;
        } finally {
          // Done: finish the full flush!
          assert Thread.holdsLock(fullFlushLock);
          docWriter.finishFullFlush(success);
          if (success) {
            processEvents(false);
            doAfterFlush();
          } else {
            if (infoStream.isEnabled("IW")) {
              infoStream.message("IW", "hit exception during NRT reader");
            }
          }
        }
      }
      if (onGetReaderMerges != null) { // only relevant if we do merge on getReader
        StandardDirectoryReader mergedReader =
            finishGetReaderMerge(
                stopCollectingMergedReaders,
                mergedReaders,
                openedReadOnlyClones,
                openingSegmentInfos,
                applyAllDeletes,
                writeAllDeletes,
                onGetReaderMerges,
                maxFullFlushMergeWaitMillis);
        if (mergedReader != null) {
          try {
            r.close();
          } finally {
            r = mergedReader;
          }
        }
      }

      anyChanges |= maybeMerge.getAndSet(false);
      if (anyChanges) {
        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);
      }
      if (infoStream.isEnabled("IW")) {
        infoStream.message(
            "IW", "getReader took " + (System.currentTimeMillis() - tStart) + " msec");
      }
      success2 = true;
    } catch (VirtualMachineError tragedy) {
      tragicEvent(tragedy, "getReader");
      throw tragedy;
    } finally {
      if (!success2) {
        try {
          IOUtils.closeWhileHandlingException(r, onGetReaderMergeResources);
        } finally {
          maybeCloseOnTragicEvent();
        }
      } else {
        IOUtils.close(onGetReaderMergeResources);
      }
    }
    return r;
  }

