  @Repeat(iterations = 10)
  // @Seed("947083AB20AB2D4F")
  public void testRandomizedRoundTrip() throws Exception {
    final int distinct = TestUtil.nextInt(random(), 1, 10);

    String[] terms = new String[distinct];
    BytesRef[] termBytes = new BytesRef[distinct];
    for (int i = 0; i < distinct; ++i) {
      terms[i] = TestUtil.randomRealisticUnicodeString(random());
      termBytes[i] = new BytesRef(terms[i]);
    }

    final BaseTermVectorsFormatTestCase.RandomTokenStream rTokenStream =
        new BaseTermVectorsFormatTestCase.RandomTokenStream(
            TestUtil.nextInt(random(), 1, 10), terms, termBytes);
    // check to see if the token streams might have non-deterministic testable result
    final boolean storeTermVectorPositions = random().nextBoolean();
    final int[] startOffsets = rTokenStream.getStartOffsets();
    final int[] positionsIncrements = rTokenStream.getPositionsIncrements();
    for (int i = 1; i < positionsIncrements.length; i++) {
      if (storeTermVectorPositions && positionsIncrements[i] != 0) {
        continue;
      }
      // TODO should RandomTokenStream ensure endOffsets for tokens at same position and same
      // startOffset are greater
      // than previous token's endOffset?  That would increase the testable possibilities.
      if (startOffsets[i] == startOffsets[i - 1]) {
        if (VERBOSE)
          System.out.println(
              "Skipping test because can't easily validate random token-stream is correct.");
        rTokenStream.close();
        return;
      }
    }

    // sanity check itself
    assertTokenStreamContents(
        rTokenStream,
        rTokenStream.getTerms(),
        rTokenStream.getStartOffsets(),
        rTokenStream.getEndOffsets(),
        rTokenStream.getPositionsIncrements());

    Directory dir = newDirectory();
    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
    FieldType myFieldType = new FieldType(TextField.TYPE_NOT_STORED);
    myFieldType.setStoreTermVectors(true);
    myFieldType.setStoreTermVectorOffsets(true);
    myFieldType.setStoreTermVectorPositions(storeTermVectorPositions);
    // payloads require positions; it will throw an error otherwise
    myFieldType.setStoreTermVectorPayloads(storeTermVectorPositions && random().nextBoolean());

    Document doc = new Document();
    doc.add(new Field("field", rTokenStream, myFieldType));
    writer.addDocument(doc);

    IndexReader reader = writer.getReader();
    writer.close();
    assertEquals(1, reader.numDocs());

    TokenStream vectorTokenStream =
        TokenSources.getTermVectorTokenStreamOrNull("field", reader.getTermVectors(0), -1);

    // sometimes check payloads
    PayloadAttribute payloadAttribute = null;
    if (myFieldType.storeTermVectorPayloads() && usually()) {
      payloadAttribute = vectorTokenStream.addAttribute(PayloadAttribute.class);
    }
    assertTokenStreamContents(
        vectorTokenStream,
        rTokenStream.getTerms(),
        rTokenStream.getStartOffsets(),
        rTokenStream.getEndOffsets(),
        myFieldType.storeTermVectorPositions() ? rTokenStream.getPositionsIncrements() : null);
    // test payloads
    if (payloadAttribute != null) {
      vectorTokenStream.reset();
      for (int i = 0; vectorTokenStream.incrementToken(); i++) {
        assertEquals(rTokenStream.getPayloads()[i], payloadAttribute.getPayload());
      }
    }

    reader.close();
    dir.close();
    rTokenStream.close();
  }

