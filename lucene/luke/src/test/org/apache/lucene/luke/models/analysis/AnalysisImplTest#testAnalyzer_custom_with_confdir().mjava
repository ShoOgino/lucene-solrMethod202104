  @Test
  public void testAnalyzer_custom_with_confdir() throws Exception {
    Path confDir = createTempDir("conf");
    Path stopFile = Files.createFile(Paths.get(confDir.toString(), "stop.txt"));
    Files.write(stopFile, "of\nthe\nby\nfor\n".getBytes(StandardCharsets.UTF_8));

    AnalysisImpl analysis = new AnalysisImpl();
    Map<String, String> tkParams = new HashMap<>();
    tkParams.put("maxTokenLen", "128");
    Map<String, String> tfParams = new HashMap<>();
    tfParams.put("ignoreCase", "true");
    tfParams.put("words", "stop.txt");
    tfParams.put("format", "wordset");
    CustomAnalyzerConfig.Builder builder =
        new CustomAnalyzerConfig.Builder("whitespace", tkParams)
            .configDir(confDir.toString())
            .addTokenFilterConfig("lowercase", Collections.emptyMap())
            .addTokenFilterConfig("stop", tfParams);
    CustomAnalyzer analyzer = (CustomAnalyzer) analysis.buildCustomAnalyzer(builder.build());
    assertEquals("org.apache.lucene.analysis.custom.CustomAnalyzer", analyzer.getClass().getName());
    assertEquals(
        "org.apache.lucene.analysis.core.WhitespaceTokenizerFactory",
        analyzer.getTokenizerFactory().getClass().getName());
    assertEquals(
        "org.apache.lucene.analysis.core.LowerCaseFilterFactory",
        analyzer.getTokenFilterFactories().get(0).getClass().getName());
    assertEquals(
        "org.apache.lucene.analysis.core.StopFilterFactory",
        analyzer.getTokenFilterFactories().get(1).getClass().getName());

    String text = "Government of the People, by the People, for the People";
    List<Analysis.Token> tokens = analysis.analyze(text);
    assertNotNull(tokens);
  }

