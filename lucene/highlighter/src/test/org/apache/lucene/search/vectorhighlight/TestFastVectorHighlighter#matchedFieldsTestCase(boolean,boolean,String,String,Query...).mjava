  private void matchedFieldsTestCase(
      boolean useMatchedFields,
      boolean fieldMatch,
      String fieldValue,
      String expected,
      Query... queryClauses)
      throws IOException {
    Document doc = new Document();
    FieldType stored = new FieldType(TextField.TYPE_STORED);
    stored.setStoreTermVectorOffsets(true);
    stored.setStoreTermVectorPositions(true);
    stored.setStoreTermVectors(true);
    stored.freeze();
    FieldType matched = new FieldType(TextField.TYPE_NOT_STORED);
    matched.setStoreTermVectorOffsets(true);
    matched.setStoreTermVectorPositions(true);
    matched.setStoreTermVectors(true);
    matched.freeze();
    doc.add(new Field("field", fieldValue, stored)); // Whitespace tokenized with English stop words
    doc.add(
        new Field("field_exact", fieldValue, matched)); // Whitespace tokenized without stop words
    doc.add(
        new Field(
            "field_super_exact", fieldValue, matched)); // Whitespace tokenized without toLower
    doc.add(new Field("field_characters", fieldValue, matched)); // Each letter is a token
    doc.add(new Field("field_tripples", fieldValue, matched)); // Every three letters is a token
    doc.add(
        new Field(
            "field_sliced",
            fieldValue.substring(
                0, // Sliced at 10 chars then analyzed just like field
                Math.min(fieldValue.length() - 1, 10)),
            matched));
    doc.add(
        new Field(
            "field_der_red",
            new CannedTokenStream( // Hacky field containing "der" and "red" at pos = 0
                token("der", 1, 0, 3), token("red", 0, 0, 3)),
            matched));

    final Map<String, Analyzer> fieldAnalyzers = new TreeMap<>();
    fieldAnalyzers.put(
        "field",
        new MockAnalyzer(
            random(), MockTokenizer.WHITESPACE, true, MockTokenFilter.ENGLISH_STOPSET));
    fieldAnalyzers.put("field_exact", new MockAnalyzer(random()));
    fieldAnalyzers.put(
        "field_super_exact", new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false));
    fieldAnalyzers.put(
        "field_characters",
        new MockAnalyzer(random(), new CharacterRunAutomaton(new RegExp(".").toAutomaton()), true));
    fieldAnalyzers.put(
        "field_tripples",
        new MockAnalyzer(
            random(), new CharacterRunAutomaton(new RegExp("...").toAutomaton()), true));
    fieldAnalyzers.put("field_sliced", fieldAnalyzers.get("field"));
    fieldAnalyzers.put(
        "field_der_red",
        fieldAnalyzers.get("field")); // This is required even though we provide a token stream
    Analyzer analyzer =
        new DelegatingAnalyzerWrapper(Analyzer.PER_FIELD_REUSE_STRATEGY) {
          public Analyzer getWrappedAnalyzer(String fieldName) {
            return fieldAnalyzers.get(fieldName);
          }
        };

    Directory dir = newDirectory();
    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer));
    writer.addDocument(doc);

    FastVectorHighlighter highlighter = new FastVectorHighlighter();
    FragListBuilder fragListBuilder = new SimpleFragListBuilder();
    FragmentsBuilder fragmentsBuilder = new ScoreOrderFragmentsBuilder();
    IndexReader reader = DirectoryReader.open(writer);
    String[] preTags = new String[] {"<b>"};
    String[] postTags = new String[] {"</b>"};
    Encoder encoder = new DefaultEncoder();
    int docId = 0;
    BooleanQuery.Builder query = new BooleanQuery.Builder();
    for (Query clause : queryClauses) {
      query.add(clause, Occur.MUST);
    }
    FieldQuery fieldQuery = new FieldQuery(query.build(), reader, true, fieldMatch);
    String[] bestFragments;
    if (useMatchedFields) {
      Set<String> matchedFields = new HashSet<>();
      matchedFields.add("field");
      matchedFields.add("field_exact");
      matchedFields.add("field_super_exact");
      matchedFields.add("field_characters");
      matchedFields.add("field_tripples");
      matchedFields.add("field_sliced");
      matchedFields.add("field_der_red");
      bestFragments =
          highlighter.getBestFragments(
              fieldQuery,
              reader,
              docId,
              "field",
              matchedFields,
              25,
              1,
              fragListBuilder,
              fragmentsBuilder,
              preTags,
              postTags,
              encoder);
    } else {
      bestFragments =
          highlighter.getBestFragments(
              fieldQuery,
              reader,
              docId,
              "field",
              25,
              1,
              fragListBuilder,
              fragmentsBuilder,
              preTags,
              postTags,
              encoder);
    }
    assertEquals(expected, bestFragments[0]);

    reader.close();
    writer.close();
    dir.close();
  }

