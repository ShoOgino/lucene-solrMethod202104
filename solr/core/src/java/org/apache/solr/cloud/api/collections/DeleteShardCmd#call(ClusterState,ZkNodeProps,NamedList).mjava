  @Override
  @SuppressWarnings({"unchecked"})
  public void call(ClusterState clusterState, ZkNodeProps message, @SuppressWarnings({"rawtypes"})NamedList results) throws Exception {
    String extCollectionName = message.getStr(ZkStateReader.COLLECTION_PROP);
    String sliceId = message.getStr(ZkStateReader.SHARD_ID_PROP);

    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);
    String collectionName;
    if (followAliases) {
      collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);
    } else {
      collectionName = extCollectionName;
    }

    log.info("Delete shard invoked");
    Slice slice = clusterState.getCollection(collectionName).getSlice(sliceId);
    if (slice == null) throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,
        "No shard with name " + sliceId + " exists for collection " + collectionName);

    // For now, only allow for deletions of Inactive slices or custom hashes (range==null).
    // TODO: Add check for range gaps on Slice deletion
    final Slice.State state = slice.getState();
    if (!(slice.getRange() == null || state == Slice.State.INACTIVE || state == Slice.State.RECOVERY
        || state == Slice.State.CONSTRUCTION) || state == Slice.State.RECOVERY_FAILED) {
      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, "The slice: " + slice.getName() + " is currently " + state
          + ". Only non-active (or custom-hashed) slices can be deleted.");
    }

    if (state == Slice.State.RECOVERY)  {
      // mark the slice as 'construction' and only then try to delete the cores
      // see SOLR-9455
      Map<String, Object> propMap = new HashMap<>();
      propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());
      propMap.put(sliceId, Slice.State.CONSTRUCTION.toString());
      propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);
      ZkNodeProps m = new ZkNodeProps(propMap);
      if (ocmh.getDistributedClusterStateUpdater().isDistributedStateUpdate()) {
        // In this DeleteShardCmd.call() method there are potentially two cluster state updates. This is the first one.
        // Even though the code of this method does not wait for it to complete, it does call the Collection API before
        // it issues the second state change below. The collection API will be doing its own state change(s), and these will
        // happen after this one (given it's for the same collection). Therefore we persist this state change
        // immediately and do not group it with the one done further down.
        // Once the Collection API is also distributed (and not only the cluster state updates), we will likely be able
        // to batch more/all cluster state updates done by this command (DeleteShardCmd). TODO SOLR-15146
        ocmh.getDistributedClusterStateUpdater().doSingleStateUpdate(DistributedClusterStateUpdater.MutatingCommand.SliceUpdateShardState, m,
            ocmh.cloudManager, ocmh.zkStateReader);
      } else {
        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));
      }
    }

    String asyncId = message.getStr(ASYNC);

    try {
      List<ZkNodeProps> replicas = getReplicasForSlice(collectionName, slice);
      CountDownLatch cleanupLatch = new CountDownLatch(replicas.size());
      for (ZkNodeProps r : replicas) {
        final ZkNodeProps replica = r.plus(message.getProperties()).plus("parallel", "true").plus(ASYNC, asyncId);
        if (log.isInfoEnabled()) {
          log.info("Deleting replica for collection={} shard={} on node={}", replica.getStr(COLLECTION_PROP), replica.getStr(SHARD_ID_PROP), replica.getStr(CoreAdminParams.NODE));
        }
        @SuppressWarnings({"rawtypes"})
        NamedList deleteResult = new NamedList();
        try {
          ((DeleteReplicaCmd)ocmh.commandMap.get(DELETEREPLICA)).deleteReplica(clusterState, replica, deleteResult, () -> {
            cleanupLatch.countDown();
            if (deleteResult.get("failure") != null) {
              synchronized (results) {
                results.add("failure", String.format(Locale.ROOT, "Failed to delete replica for collection=%s shard=%s" +
                    " on node=%s", replica.getStr(COLLECTION_PROP), replica.getStr(SHARD_ID_PROP), replica.getStr(NODE_NAME_PROP)));
              }
            }
            @SuppressWarnings({"rawtypes"})
            SimpleOrderedMap success = (SimpleOrderedMap) deleteResult.get("success");
            if (success != null) {
              synchronized (results)  {
                results.add("success", success);
              }
            }
          });
        } catch (KeeperException e) {
          log.warn("Error deleting replica: {}", r, e);
          cleanupLatch.countDown();
        } catch (Exception e) {
          log.warn("Error deleting replica: {}", r, e);
          cleanupLatch.countDown();
          throw e;
        }
      }
      log.debug("Waiting for delete shard action to complete");
      cleanupLatch.await(1, TimeUnit.MINUTES);

      ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, DELETESHARD.toLower(), ZkStateReader.COLLECTION_PROP,
          collectionName, ZkStateReader.SHARD_ID_PROP, sliceId);
      ZkStateReader zkStateReader = ocmh.zkStateReader;
      if (ocmh.getDistributedClusterStateUpdater().isDistributedStateUpdate()) {
        ocmh.getDistributedClusterStateUpdater().doSingleStateUpdate(DistributedClusterStateUpdater.MutatingCommand.CollectionDeleteShard, m,
            ocmh.cloudManager, ocmh.zkStateReader);
      } else {
        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));
      }

      zkStateReader.waitForState(collectionName, 45, TimeUnit.SECONDS, (c) -> c.getSlice(sliceId) == null);

      log.info("Successfully deleted collection: {} , shard: {}", collectionName, sliceId);
    } catch (SolrException e) {
      throw e;
    } catch (Exception e) {
      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,
          "Error executing delete operation for collection: " + collectionName + " shard: " + sliceId, e);
    }
  }

