  public void testCaching() throws IOException {
    Directory dir = newDirectory();
    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
    Document doc = new Document();
    AtomicInteger resetCount = new AtomicInteger(0);
    TokenStream stream =
        new TokenStream() {
          private int index = 0;
          private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
          private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);

          @Override
          public void reset() throws IOException {
            super.reset();
            resetCount.incrementAndGet();
          }

          @Override
          public boolean incrementToken() {
            if (index == tokens.length) {
              return false;
            } else {
              clearAttributes();
              termAtt.append(tokens[index++]);
              offsetAtt.setOffset(0, 0);
              return true;
            }
          }
        };

    stream = new CachingTokenFilter(stream);

    doc.add(new TextField("preanalyzed", stream));

    // 1) we consume all tokens twice before we add the doc to the index
    assertFalse(((CachingTokenFilter) stream).isCached());
    stream.reset();
    assertFalse(((CachingTokenFilter) stream).isCached());
    checkTokens(stream);
    stream.reset();
    checkTokens(stream);
    assertTrue(((CachingTokenFilter) stream).isCached());

    // 2) now add the document to the index and verify if all tokens are indexed
    //    don't reset the stream here, the DocumentWriter should do that implicitly
    writer.addDocument(doc);

    IndexReader reader = writer.getReader();
    PostingsEnum termPositions =
        MultiTerms.getTermPostingsEnum(reader, "preanalyzed", new BytesRef("term1"));
    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);
    assertEquals(1, termPositions.freq());
    assertEquals(0, termPositions.nextPosition());

    termPositions = MultiTerms.getTermPostingsEnum(reader, "preanalyzed", new BytesRef("term2"));
    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);
    assertEquals(2, termPositions.freq());
    assertEquals(1, termPositions.nextPosition());
    assertEquals(3, termPositions.nextPosition());

    termPositions = MultiTerms.getTermPostingsEnum(reader, "preanalyzed", new BytesRef("term3"));
    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);
    assertEquals(1, termPositions.freq());
    assertEquals(2, termPositions.nextPosition());
    reader.close();
    writer.close();
    // 3) reset stream and consume tokens again
    stream.reset();
    checkTokens(stream);

    assertEquals(1, resetCount.get());

    dir.close();
  }

