  @Ignore(
      "This test is used during development when analyze normalizations in large amounts of text")
  @Test
  public void testLargeData() throws IOException {
    Path input = Paths.get("/tmp/test.txt");
    Path tokenizedOutput = Paths.get("/tmp/test.tok.txt");
    Path normalizedOutput = Paths.get("/tmp/test.norm.txt");

    Analyzer plainAnalyzer =
        new Analyzer() {
          @Override
          protected TokenStreamComponents createComponents(String fieldName) {
            UserDictionary userDictionary = readDict();
            Set<POS.Tag> stopTags = new HashSet<>();
            stopTags.add(POS.Tag.SP);
            Tokenizer tokenizer =
                new KoreanTokenizer(
                    newAttributeFactory(),
                    userDictionary,
                    KoreanTokenizer.DEFAULT_DECOMPOUND,
                    false,
                    false);
            return new TokenStreamComponents(
                tokenizer, new KoreanPartOfSpeechStopFilter(tokenizer, stopTags));
          }
        };

    analyze(
        plainAnalyzer,
        Files.newBufferedReader(input, StandardCharsets.UTF_8),
        Files.newBufferedWriter(tokenizedOutput, StandardCharsets.UTF_8));

    analyze(
        analyzer,
        Files.newBufferedReader(input, StandardCharsets.UTF_8),
        Files.newBufferedWriter(normalizedOutput, StandardCharsets.UTF_8));
    plainAnalyzer.close();
  }

