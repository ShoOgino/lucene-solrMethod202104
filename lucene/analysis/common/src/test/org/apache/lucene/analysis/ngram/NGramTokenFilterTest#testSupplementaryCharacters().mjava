  public void testSupplementaryCharacters() throws IOException {
    for (int i = 0; i < 20; i++) {
      final String s = TestUtil.randomUnicodeString(random(), 10);
      final int codePointCount = s.codePointCount(0, s.length());
      final int minGram = TestUtil.nextInt(random(), 1, 3);
      final int maxGram = TestUtil.nextInt(random(), minGram, 10);
      final boolean preserveOriginal = TestUtil.nextInt(random(), 0, 1) % 2 == 0;

      TokenStream tk = new KeywordTokenizer();
      ((Tokenizer)tk).setReader(new StringReader(s));
      tk = new NGramTokenFilter(tk, minGram, maxGram, preserveOriginal);
      final CharTermAttribute termAtt = tk.addAttribute(CharTermAttribute.class);
      final OffsetAttribute offsetAtt = tk.addAttribute(OffsetAttribute.class);
      tk.reset();

      if (codePointCount < minGram && preserveOriginal) {
        assertTrue(tk.incrementToken());
        assertEquals(0, offsetAtt.startOffset());
        assertEquals(s.length(), offsetAtt.endOffset());
        assertEquals(s, termAtt.toString());
      }
      
      for (int start = 0; start < codePointCount; ++start) {
        for (int end = start + minGram; end <= Math.min(codePointCount, start + maxGram); ++end) {
          assertTrue(tk.incrementToken());
          assertEquals(0, offsetAtt.startOffset());
          assertEquals(s.length(), offsetAtt.endOffset());
          final int startIndex = Character.offsetByCodePoints(s, 0, start);
          final int endIndex = Character.offsetByCodePoints(s, 0, end);
          assertEquals(s.substring(startIndex, endIndex), termAtt.toString());
        }
      }
      
      if (codePointCount > maxGram && preserveOriginal) {
        assertTrue(tk.incrementToken());
        assertEquals(0, offsetAtt.startOffset());
        assertEquals(s.length(), offsetAtt.endOffset());
        assertEquals(s, termAtt.toString());
      }
      
      assertFalse(tk.incrementToken());
      tk.close();
    }
  }

