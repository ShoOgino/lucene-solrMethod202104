    /**
     * After the computing of the new {@link ClusterState} containing all needed updates to the collection based on what the
     * {@link StateChangeCalculator} computed, this method does an update in ZK to the collection's {@code state.json}. It is the
     * equivalent of Overseer's {@link ZkStateWriter#writePendingUpdates} (in its actions related to {@code state.json}
     * as opposed to the per replica states).
     * <p>
     * Note that in a similar way to what happens in {@link ZkStateWriter#writePendingUpdates}, collection delete is handled
     * as a special case. (see comment on {@link DistributedClusterStateUpdater.StateChangeRecorder.RecordedMutationsPlayer}
     * on why the code has to be duplicated)<p>
     *
     * <b>Note for the future:</b> Given this method is where the actually write to ZK is done, that's the place where we
     * can rebuild a DocCollection with updated zk version. Eventually if we maintain a cache of recently used collections,
     * we want to capture the updated collection and put it in the cache to avoid reading it again (unless it changed,
     * the CAS will fail and we will refresh).<p>
     *
     * This could serve as the basis for a strategy where each node does not need any view of all collections in the cluster
     * but only a cache of recently used collections (possibly not even needing watches on them, but we'll discuss this later).
     */
    private void doStateDotJsonCasUpdate(ClusterState updatedState) throws KeeperException, InterruptedException {
      String jsonPath = ZkStateReader.getCollectionPath(updater.getCollectionName());

      // Collection delete
      if (!updatedState.hasCollection(updater.getCollectionName())) {
        // We do not have a collection znode version to test we delete the right version of state.json. But this doesn't really matter:
        // if we had one, and the delete failed (because state.json got updated in the meantime), we would re-read the collection
        // state, update our version, run the CAS delete again and it will pass. Which means that one way or another, deletes are final.
        // I hope nobody deletes a collection then creates a new one with the same name immediately (although the creation should fail
        // if the znode still exists, so the creation would only succeed after the delete made it, and we're ok).
        // With Overseer based updates the same behavior can be observed: a collection update is enqueued followed by the
        // collection delete before the update was executed.
        log.debug("going to recursively delete state.json at {}", jsonPath);
        zkStateReader.getZkClient().clean(jsonPath);
      } else {
        // Collection update or creation
        DocCollection collection = updatedState.getCollection(updater.getCollectionName());
        byte[] stateJson = Utils.toJSON(singletonMap(updater.getCollectionName(), collection));

        if (updater.isCollectionCreation()) {
          // The state.json file does not exist yet (more precisely it is assumed not to exist)
          log.debug("going to create collection {}", jsonPath);
          zkStateReader.getZkClient().create(jsonPath, stateJson, CreateMode.PERSISTENT, true);
        } else {
          // We're updating an existing state.json
          if (log.isDebugEnabled()) {
            log.debug("going to update collection {} version: {}", jsonPath, collection.getZNodeVersion());
          }
          zkStateReader.getZkClient().setData(jsonPath, stateJson, collection.getZNodeVersion(), true);
        }
      }
    }

